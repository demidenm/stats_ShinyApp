X1 = log_dat$x1,
fitted = predict(mod, y))
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y)
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y,
X1 = log_dat$x1,
X2 = log_dat$x2)
fitted = predict(mod, y)
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y,
X1 = log_dat$x1,
X2 = log_dat$x2,
fitted = predict(mod, log_dat$y))
predict(object = mod, y)
predict.glm(object = mod, newdata = log_dat$y, type = "response")
mod <- glm(y ~ x1 + x2, data = log_dat, family = "binomial")
summary(mod)
predict.glm(object = mod, newdata = log_dat$y, type = "response")
out_log <- glm(y ~ x1 + x2, data = log_dat, family = "binomial")
summary(out_log)
exp(coef(out_log))
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y,
X1 = log_dat$x1,
X2 = log_dat$x2,
fitted = pred(mod, log_dat$y))
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y,
X1 = log_dat$x1,
X2 = log_dat$x2,
fitted = predict(mod, log_dat$y))
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y,
X1 = log_dat$x1,
X2 = log_dat$x2,
fitted = predict(out_log, log_dat$y))
# Logistic Regression
set.seed(123)
n = 500
beta_0  = .75
beta_1= .16
beta_2 = .33
x1 = rnorm(n=n)
x2 = rnorm(n=n)
z = beta_0 + beta_1*x1 + beta_2*x2
pr = 1 / (1 + exp(-z))
y = rbinom(n = n, size = 1, prob = pr)
log_dat <- data.frame(y = y, x1 = x1, x2 = x2)
out_log <- glm(y ~ x1 + x2, data = log_dat, family = "binomial")
summary(out_log)
exp(coef(out_log))
# set up data w/ predicted values
logistic_plot <- data.frame("prob" = log_dat$y,
X1 = log_dat$x1,
X2 = log_dat$x2,
fitted = predict(out_log, log_dat$y))
source("logit_dotplot.R")
p <- predict(out_log,
newdata = log_dat$y,
type = "response",
se.fit = TRUE)
out_log <- glm(y ~ x1, data = log_dat, family = "binomial")
p <- predict(out_log,
newdata = log_dat$y,
type = "response",
se.fit = TRUE)
out_log <- glm(y ~ x1 + x2, data = log_dat, family = "binomial")
summary(out_log)
p <- predict(out_log,
newdata = log_dat$y,
type = "response",
se.fit = TRUE)
p <- predict(out_log,
newdata = c(x1,x2),
type = "response",
se.fit = TRUE)
p <- predict(out_log,
newdata = c("x1","x2"),
type = "response",
se.fit = TRUE)
p <- predict(out_log,
newdata = c(x1,x2),
type = "link",
se.fit = TRUE)
p <- predict(out_log,
newdata = log_dat$y,
type = "link",
se.fit = TRUE)
devtools::install_github("lnalborczyk/lmisc", dependencies = TRUE)
p <- predict(out_log,
newdata = c(log_dat$x1, log_dat$x2),
type = "link",
se.fit = TRUE)
source("https://rdrr.io/github/lnalborczyk/lmisc/src/R/logit_dotplot.R")
type(log_dat$y)
typeof(log_dat$y)
p <- predict(out_log,
newdata = subset(log_dat, select=c(x1, x2)),
type = "link",
se.fit = TRUE)
View(p)
p <- predict(out_log,
newdata = subset(log_dat, select=c(x1, x2)),
type = "link",
se.fit = TRUE) %>%
data.frame() %>%
mutate(ll = fit - 1.96 * se.fit,
ul = fit + 1.96 * se.fit) %>%
select(-residual.scale, -se.fit) %>%
mutate_all(plogis) %>%
bind_cols(nd)
library(tidyverse)
p <- predict(out_log,
newdata = subset(log_dat, select=c(x1, x2)),
type = "link",
se.fit = TRUE) %>%
data.frame() %>%
mutate(ll = fit - 1.96 * se.fit,
ul = fit + 1.96 * se.fit) %>%
select(-residual.scale, -se.fit) %>%
mutate_all(plogis) %>%
bind_cols(nd)
p <- predict(out_log,
newdata = subset(log_dat, select=c(x1, x2)),
type = "link",
se.fit = TRUE) %>%
data.frame() %>%
mutate(ll = fit - 1.96 * se.fit,
ul = fit + 1.96 * se.fit) %>%
select(-residual.scale, -se.fit) %>%
mutate_all(plogis) %>%
bind_cols(log_dat)
View(log_dat)
View(p)
p %>%
ggplot(aes(x = x1)) +
geom_ribbon(aes(ymin = ll, ymax = ul),
alpha = .5)+
geom_line(aes(y = fit)) +
stat_dots(data = log_dat %>%
mutate(binary = factor(y, levels = c("Win", "Lose"))),
aes(y = y,
side = ifelse(Win == 0, "top", "bottom"),
color = binary),
scale = .4, shape = 19)
library(devtools)
source("logit_dotplot.R")
source("https://rdrr.io/github/lnalborczyk/lmisc/src/R/logit_dotplot.R")
p %>%
ggplot(aes(x = x1)) +
geom_ribbon(aes(ymin = ll, ymax = ul),
alpha = 1/2) +
geom_line(aes(y = fit)) +
stat_dots(data = log_dat,
aes(y = y, side = ifelse(0 == 0, "Win", "Loss")),
scale = 1/3) +
scale_y_continuous("probability of passing",
expand = c(0, 0))
source("https://github.com/lnalborczyk/lnalborczyk.github.io/blob/master/code/logit_dotplot.R")
logit_dotplot <- function(x, y, xlab, ylab) {
# inspired from: https://recology.info/2012/01/logistic-regression-barplot-fig/
# and: http://rpubs.com/kohske/dual_axis_in_ggplot2
# NB: x should be a continuous predictor
# NB: y should be a dichotomic outcome
library(tidyverse)
library(gtable)
library(grid)
if (length(unique(y) ) > 2)
stop("\n y should be a dichotomic outcome \n")
d <- data.frame(x = x, y = y)
a <-
d %>%
ggplot(aes(x = x, y = y) ) +
geom_smooth(
method = "glm",
method.args = list(family = "binomial"),
se = FALSE, color = "black") +
geom_dotplot(
data = d[d$y == 0, ],
stackdir = "up",
binwidth = 0.5, dotsize = 1, alpha = 0.2,
show.legend = FALSE) +
scale_x_continuous(limits = range(d$x) ) +
theme_bw(base_size = 12) +
labs(x = xlab, y = ylab)
b <-
d %>%
ggplot(aes(x = x, y = y) ) +
geom_dotplot(
data = d[d$y == 1, ],
stackdir = "down",
binwidth = 0.5, dotsize = 1, alpha = 0.2,
show.legend = FALSE) +
scale_x_continuous(limits = range(d$x) ) +
scale_y_continuous(trans = "reverse", limits = c(1, 0) ) +
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text.y = element_blank(),
axis.text.x = element_blank(),
axis.ticks = element_blank(),
panel.border = element_blank(),
panel.background = element_blank() )
# extract gtable
g1 <- ggplot_gtable(ggplot_build(a) )
g2 <- ggplot_gtable(ggplot_build(b) )
# overlap the panel of 2nd plot on that of 1st plot
pp <- c(subset(g1$layout, name == "panel", se = t:r) )
g <-
gtable_add_grob(
g1, g2$grobs[[which(g2$layout$name == "panel")]],
pp$t, pp$l, pp$b, pp$l)
# plot a new viewports
vp = viewport(width = 1, height = 1, x = 0.5, y = 0.5)
pushViewport(vp)
grid.draw(g)
upViewport()
}
install.packages("ggdist")
library(ggdist)
library(tidyverse)
p %>%
ggplot(aes(x = x1)) +
geom_ribbon(aes(ymin = ll, ymax = ul),
alpha = 1/2) +
geom_line(aes(y = fit)) +
stat_dots(data = log_dat,
aes(y = y, side = ifelse(0 == 0, "Win", "Loss")),
scale = 1/3) +
scale_y_continuous("probability of passing",
expand = c(0, 0))
p %>%
ggplot(aes(x = x1)) +
geom_ribbon(aes(ymin = ll, ymax = ul),
alpha = 1/2) +
geom_line(aes(y = fit)) +
stat_dots(data = log_dat,
aes(y = y),
scale = 1/3) +
scale_y_continuous("probability of passing",
expand = c(0, 0))
p %>%
ggplot(aes(x = x1)) +
geom_ribbon(aes(ymin = ll, ymax = ul),
alpha = 1/2) +
geom_line(aes(y = fit)) +
stat_dots(data = log_dat,
aes(y = y),
scale = 1) +
scale_y_continuous("probability of passing",
expand = c(0, 0))
p %>%
ggplot(aes(x = x1)) +
geom_ribbon(aes(ymin = ll, ymax = ul),
alpha = 1/2) +
geom_line(aes(y = fit)) +
stat_dots(data = log_dat,
aes(y = y)) +
scale_y_continuous("probability of passing",
expand = c(0, 0))
library(MASS) # exports mvrnorm()
#
# Describe the independent variables.
#
mu <- c(0,0,0,0)
Sigma <- cbind(c(10,6,5,6), c(6,10,5,3.5), c(5,5,10,3), c(6,3.5,3,10))/10
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -3.37154                  # Corresponds to 5%.
n.obs <- 1e5
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -3.37154                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
y <- runif(n.obs) < 1 / (1 + exp(-beta.0 - x %*% beta))
View(y)
#
# Confirm that the independent variables have the desired correlation
# and the dependent variable has the desired proportion of true responses.
#
round(cor(x), 2)
mean(y)
#
# Describe the independent variables.
#
mu <- c(0,0,0,0)
Sigma <- cbind(c(10,6,5,6), c(6,10,5,3.5), c(5,5,10,3), c(6,3.5,3,10))/10
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -4                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
y <- runif(n.obs) < 1 / (1 + exp(-beta.0 - x %*% beta))
#
# Confirm that the independent variables have the desired correlation
# and the dependent variable has the desired proportion of true responses.
#
round(cor(x), 2)
mean(y)
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -25                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
y <- runif(n.obs) < 1 / (1 + exp(-beta.0 - x %*% beta))
#
# Confirm that the independent variables have the desired correlation
# and the dependent variable has the desired proportion of true responses.
#
round(cor(x), 2)
mean(y)
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -.75                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
y <- runif(n.obs) < 1 / (1 + exp(-beta.0 - x %*% beta))
#
# Confirm that the independent variables have the desired correlation
# and the dependent variable has the desired proportion of true responses.
#
round(cor(x), 2)
mean(y)
Sigma <- cbind(c(1,.6,.5), c(.6,1,.5), c(.6,.5,10))
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -.75                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
Sigma <- cbind(c(1,.6,.5), c(.6,1,.5), c(.6,.5,1))
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -.75                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
#
# Describe the independent variables.
#
mu <- c(0,0,0)
Sigma <- cbind(c(10,6,5,6), c(6,10,5,3.5), c(5,5,10,3), c(6,3.5,3,10))/10
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -.75                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
#
# Describe the independent variables.
#
mu <- c(0,0,0,0)
Sigma <- cbind(c(10,6,5,6), c(6,10,5,3.5), c(5,5,10,3), c(6,3.5,3,10))/10
#
# Simulate a dataset in which Pr(Y=1) is 5%.
#
beta.0 <- -3.37154                  # Corresponds to 5%.
n.obs <- 100
x <- mvrnorm(n.obs, mu, Sigma)
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
y <- runif(n.obs) < 1 / (1 + exp(-beta.0 - x %*% beta))
data.frame(x, y)
dat_log <- data.frame(x, y)
View(log_dat)
View(dat_log)
log_mod <- glm(y ~ X1 + X1, family = "binomial", data = dat_log)
summary(log_mod)
log_mod <- glm(y ~ X1 + X2, family = "binomial", data = dat_log)
summary(log_mod)
log_mod <- glm(y ~ X1 + X2 + X3, family = "binomial", data = dat_log)
summary(log_mod)
log_mod <- glm(y ~ X1 + X2 + X3 + X4, family = "binomial", data = dat_log)
summary(log_mod)
cor(x)
y <- runif(n.obs, min = 0, max = 3) < 1 / (1 + exp(-beta.0 - x %*% beta))
#
# Confirm that the independent variables have the desired correlation
# and the dependent variable has the desired proportion of true responses.
#
round(cor(x), 2)
mean(y)
dat_log <- data.frame(x, y)
log_mod <- glm(y ~ X1 + X2 + X3 + X4, family = "binomial", data = dat_log)
summary(log_mod)
cor(x)
y <- runif(n.obs, min = 0, max = 3) *.32 < 1 / (1 + exp(-beta.0 - x %*% beta))
#
# Confirm that the independent variables have the desired correlation
# and the dependent variable has the desired proportion of true responses.
#
round(cor(x), 2)
mean(y)
dat_log <- data.frame(x, y)
log_mod <- glm(y ~ X1 + X2 + X3 + X4, family = "binomial", data = dat_log)
summary(log_mod)
cor(x)
x <- as.data.frame(mvrnorm(n.obs, mu, Sigma))
beta <- rnorm(4)                    # Can be anything (nonzero)!
sigma2 <- beta %*% Sigma %*% beta
beta <- beta / sqrt(sigma2)         # Assures sum of squares is unity
y <- runif(n.obs, min = 0, max = 3) * x$V1 < 1 / (1 + exp(-beta.0 - x %*% beta))
shiny::runApp('Desktop/UM/shiny_app/stats_ShinyApp')
runApp()
lm.beta
library(lm.beta)
library(Quantpsych
)
library("Quantpsych")
library("Quantpsyc")
install.packages("QuantPsyc")
library("Quantpsyc")
library(QuantPsyc)
library(MASS) # exports mvrnorm()
m_reg_df <-
data.frame(
mvrnorm(n = 100,
mu = c(5,2,4),
Sigma = matrix(c(1, .21, .33,
.21,1,.15,
.33,.15,1),
nrow = 3),
empirical = TRUE))
m_reg_df<- rename(m_reg_df,
c("X1"="Drinking", "X2" ="Reward_Seek","X3"="Age"))
mod <- lm(Drinking ~ Age + Reward_Seek, data = m_reg_df)
summary(mod)
lm.beta(MOD = mod)
lm.beta(MOD = mod)[2,1]
lm.beta(MOD = mod)[1,2]
lm.beta(MOD = mod)[1]
lm.beta(MOD = mod)[2]
as.numeric(lm.beta(MOD = mod)[1])
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
summary(mod)
summary(mod)$cofficients
summary(mod)$coefficients
summary(mod)$fstatistic
summary(mod)$coefficients[2,3]
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
mod <- lm(Drinking ~ Reward_Seek, data = m_reg_df)
summary(mod)$coefficients[2,3]
summary(mod)
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
m_reg_df <-
data.frame(
mvrnorm(n = 322,
mu = c(.5,7.5,-2.7),
Sigma = matrix(c(1, .135, .45,
.135,1,.41,
.45,.41,1),
nrow = 3),
empirical = TRUE))
summary(mod)
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
summary(mod)$coefficients
round(summary(mod)$coefficients,5)
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
summary(mod)$coefficients %>%  kable()
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
summary(mod)$coefficients %>%  kable(row.names = c("1","2"))
summary(mod)$coefficients %>%  kable(row.names = c("","1","2"))
summary(mod)$coefficients %>%  nrow()
summary(mod)$coefficients %>%  kable(row.names = c("1" = "Intercept","2"="Reward_Seek"))
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
m_reg_df %>%
summarise("N" = n(), "Drinking Mean" = mean(Drinking), "Drinking SD" = sd(Drinking),
"Reward Seek Mean" = mean(Reward_Seek), "Reward Seek SD" = sd(Reward_Seek)) %>%
kable(caption = "Table 1. Descriptive Statistics for Simulated Data")
dplyr::summarise("N" = n(), "Drinking Mean" = mean(Drinking), "Drinking SD" = sd(Drinking),
"Reward Seek Mean" = mean(Reward_Seek), "Reward Seek SD" = sd(Reward_Seek)) %>%
kable(caption = "Table 1. Descriptive Statistics for Simulated Data")
m_reg_df %>% summarize("N" = n(), "Drinking Mean" = mean(Drinking), "Drinking SD" = sd(Drinking),
"Reward Seek Mean" = mean(Reward_Seek), "Reward Seek SD" = sd(Reward_Seek)) %>%
kable(caption = "Table 1. Descriptive Statistics for Simulated Data")
m_reg_df %>% dplyr::summarize("N" = n(), "Drinking Mean" = mean(Drinking), "Drinking SD" = sd(Drinking),
"Reward Seek Mean" = mean(Reward_Seek), "Reward Seek SD" = sd(Reward_Seek)) %>%
kable(caption = "Table 1. Descriptive Statistics for Simulated Data")
View(m_reg_df)
runApp('Desktop/UM/shiny_app/stats_ShinyApp')
